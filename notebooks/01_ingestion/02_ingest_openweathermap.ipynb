{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "167d7ac8-8827-4bee-aa44-d253640664d2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set up urls"
    }
   },
   "outputs": [],
   "source": [
    "%run \"../00_config/01_urls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e9fed9a-6e3a-4092-9c15-98ad1287e514",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set up adls config"
    }
   },
   "outputs": [],
   "source": [
    "%run \"../00_config/00_adls_config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1a92b95-59a8-4e39-850a-a4c8652cadc4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load cities"
    }
   },
   "outputs": [],
   "source": [
    "def get_newest_path(base_path):\n",
    "    current_path = base_path\n",
    "    while True:\n",
    "        try:\n",
    "            contents = dbutils.fs.ls(current_path)\n",
    "        except Exception:\n",
    "            return current_path\n",
    "        if not contents or any('_delta_log' in path.path for path in contents):\n",
    "            return current_path\n",
    "        current_path = max(contents, key=lambda x: x.name).path\n",
    "\n",
    "cities_base_path = f\"{SILVER_LAYER_PATH}city_data_cleaned/\"\n",
    "latest_date_path = get_newest_path(cities_base_path)\n",
    "try:    \n",
    "    df_cities = spark.read.format(\"delta\").load(latest_date_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Delta table: {e}\")\n",
    "\n",
    "print(\"City data loaded from Silver Layer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8454fb9-2f83-4259-b180-3072c3ef54fa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fetch weather"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField, LongType, FloatType, DoubleType\n",
    "from pyspark.sql.functions import col, udf, from_json, explode, current_timestamp, from_unixtime\n",
    "\n",
    "api_key = dbutils.secrets.get(scope=\"openweathermap\", key=\"api-key\")\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def get_weather(latitude, longitude):\n",
    "    url = f\"{OPENWEATHER_API_BASE_URL}?lat={latitude}&lon={longitude}&appid={api_key}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return json.dumps(response.json())\n",
    "        else:\n",
    "            print(f\"API Error for lat={latitude}, lon={longitude}: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "    \n",
    "df_with_json = df_cities.withColumn(\"weather_json_raw\", get_weather(col(\"latitude\"), col(\"longitude\")))\n",
    "\n",
    "weather_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"id\", LongType(), True),\n",
    "        StructField(\"main\", StringType(), True),\n",
    "        StructField(\"description\", StringType(), True),\n",
    "        StructField(\"icon\", StringType(), True),\n",
    "    ])\n",
    ")\n",
    "\n",
    "hourly_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"dt\", LongType(), True),\n",
    "        StructField(\"temp\", DoubleType(), True),\n",
    "        StructField(\"feels_like\", DoubleType(), True),\n",
    "        StructField(\"pressure\", LongType(), True),\n",
    "        StructField(\"humidity\", LongType(), True),\n",
    "        StructField(\"wind_speed\", DoubleType(), True),\n",
    "        StructField(\"wind_deg\", LongType(), True),\n",
    "        StructField(\"clouds\", LongType(), True),\n",
    "        StructField(\"rain\", StructType([StructField(\"1h\", FloatType(), True)]), True),\n",
    "        StructField(\"weather\", weather_schema, True),\n",
    "    ])\n",
    ")\n",
    "df_raw_weather = (\n",
    "    df_with_json\n",
    "    .withColumn(\"weather_records\", from_json(col(\"weather_json_raw\"), StructType([\n",
    "        StructField(\"hourly\", hourly_schema),\n",
    "        StructField(\"timezone\", LongType(), True),\n",
    "        StructField(\"timezone_offset\", LongType(), True),\n",
    "    ])))\n",
    "    .withColumn(\"record\", explode(col(\"weather_records.hourly\")))\n",
    "    .withColumn(\"dt_iso\",from_unixtime(col(\"record.dt\"), \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "    .select(\n",
    "        col(\"country\"),\n",
    "        col(\"state\"),\n",
    "        col(\"latitude\"),\n",
    "        col(\"longitude\"),\n",
    "        col(\"weather_records.timezone\"),\n",
    "        col(\"weather_records.timezone_offset\"),\n",
    "        col(\"dt_iso\"),\n",
    "        col(\"record.temp\"),\n",
    "        col(\"record.feels_like\"),\n",
    "        col(\"record.pressure\"),\n",
    "        col(\"record.humidity\"),\n",
    "        col(\"record.wind_speed\"),\n",
    "        col(\"record.wind_deg\"),\n",
    "        col(\"record.clouds\"),\n",
    "        col(\"record.weather\").getItem(0).getField(\"main\").alias(\"weather_main\"),\n",
    "        col(\"record.weather\").getItem(0).getField(\"description\").alias(\"weather_description\"),\n",
    "        col(\"record.rain.1h\").alias(\"rain\"),\n",
    "        current_timestamp().alias(\"ingestion_timestamp\") # ThÃªm ingestion_timestamp\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfecaa1d-c661-4daf-98c7-fdb7d787948f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write raw weather data"
    }
   },
   "outputs": [],
   "source": [
    "# --- Save raw data to Bronze Layer ---\n",
    "current_date_path = datetime.now().strftime(\"%Y/%m/%d\")\n",
    "bronze_output_path = f\"{BRONZE_LAYER_PATH}openweathermap_hourly_raw/dt={current_date_path}/\"\n",
    "\n",
    "print(f\"Saving raw weather data to Bronze Layer at: {bronze_output_path}\")\n",
    "\n",
    "df_raw_weather.write.format(\"delta\").mode(\"append\").save(bronze_output_path)\n",
    "\n",
    "print(\"Raw weather data saved to Delta Lake in Bronze Layer.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_ingest_openweathermap",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
